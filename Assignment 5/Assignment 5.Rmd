---
title: "Assignment 5"
author: "Rohith Desamseety"
date: "2022-11-30"
output: pdf_document
---
```{r}
#Importing the necessary packages and libraries
library(cluster)
library(caret)
library(dendextend)
library(knitr)
library(factoextra)
library(readr)
```

```{r}
#Importing a dataset and producing a data set that only contains numerical information
library(readr)
Cereals <- read.csv("~/Downloads/Cereals.csv")
View(Cereals)
Numerical_data <- data.frame(Cereals[,4:16])
```

```{r}
#Removing missing values from the data
Numerical_data <- na.omit(Numerical_data)
```

```{r}
#Data normalization
Data_Cereals_normalise <- scale(Numerical_data)
```

```{r}
#Using the Euclidean distance algorithm on the normalized data, apply hierarchical clustering to the data.
Distance <- dist(Data_Cereals_normalise, method = "euclidean")
Hierarchial_Clustering <- hclust(Distance, method = "complete")
```

```{r}
#Creating a dendogram plot.
plot(Hierarchial_Clustering, cex = 0.7, hang = -1)
```

```{r}
#Using Agnes function to perform clustering with single linkage, complete linkage
#,average linkage and Ward.
HC_single <- agnes(Data_Cereals_normalise, method = "single")
HC_complete <- agnes(Data_Cereals_normalise, method = "complete")
HC_average <- agnes(Data_Cereals_normalise, method = "average")
HC_ward <- agnes(Data_Cereals_normalise, method = "ward")
```

```{r}
#Choosing the most effective strategy
print(HC_single$ac)
print(HC_complete$ac)
print(HC_average$ac)
print(HC_ward$ac)
```
#Given the information above, it is clear that the ward approach is the most effective because of its value of 0.9046042.

***#Task 2-  Choosing the clusters:***
```{r}
pltree(HC_ward, cex = 0.5, hang = -1, main = "Dendrogram of agnes (Using Ward)")
rect.hclust(HC_ward, k = 5, border = 2:7)
Group <- cutree(HC_ward, k=5)
dataframe2 <- as.data.frame(cbind(Data_Cereals_normalise,Group))
```
```{r}
fviz_cluster(list(data = dataframe2, cluster = Group))
```
#5 clusters can be chosen from the observation above..
#figuring out the clusters stability and structure. 

```{r}
#Constructing Partitions
set.seed(123)
Partition_1 <- Numerical_data[1:50,]
Partition_2 <- Numerical_data[51:74,]
```

```{r}
#Performing Hierarchical Clustering, taking into account k = 5.
RD_single <- agnes(scale(Partition_1), method = "single")
RD_complete <- agnes(scale(Partition_1), method = "complete")
RD_average <- agnes(scale(Partition_1), method = "average")
RD_ward <- agnes(scale(Partition_1), method = "ward")
cbind(single=RD_single$ac , complete=RD_complete$ac , average= RD_average$ac , ward= RD_ward$ac)
pltree(RD_ward, cex = 0.6, hang = -1, main = "Dendogram of Agnes with Partitioned Data (Using Ward)")
rect.hclust(RD_ward, k = 5, border = 2:7)
cut_2 <- cutree(RD_ward, k = 5)
```

```{r}
#Calculating the centroids.
result <- as.data.frame(cbind(Partition_1, cut_2))
result[result$cut_2==1,]
centroid_1 <- colMeans(result[result$cut_2==1,])
result[result$cut_2==2,]
centroid_2 <- colMeans(result[result$cut_2==2,])
result[result$cut_2==3,]
centroid_3 <- colMeans(result[result$cut_2==3,])
result[result$cut_2==4,]
centroid_4 <- colMeans(result[result$cut_2==4,])
centroids <- rbind(centroid_1, centroid_2, centroid_3, centroid_4)
x2 <- as.data.frame(rbind(centroids[,-14], Partition_2))
```

```{r}
#Calculating the Distance.
Distance_1 <- get_dist(x2)
Matrix_1 <- as.matrix(Distance_1)
dataframe1 <- data.frame(data=seq(1,nrow(Partition_2),1), Clusters = rep(0,nrow(Partition_2)))
for(i in 1:nrow(Partition_2)) 
  {dataframe1[i,2] <- which.min(Matrix_1[i+4, 1:4])}
dataframe1
cbind(dataframe2$Group[51:74], dataframe1$Clusters)
table(dataframe2$Group[51:74] == dataframe1$Clusters)
```
#Our results from the observation above are 12 False and 12 True. As a result, we may say that the model is just partly unstable.

***#TASK 3- The elementary public schools would like to choose a set of cereals to include in their daily cafeterias. Every day a different cereal is offered, but all cereals should support a healthy diet. For this goal, you are requested to find a cluster of “healthy cereals.”***

```{r}
#Clustering Healthy Cereals.
Healthy_Cereals <- Cereals
Healthy_Cereals_RD <- na.omit(Healthy_Cereals)
clust <- cbind(Healthy_Cereals_RD, Group)
clust[clust$Group==1,]
clust[clust$Group==2,]
clust[clust$Group==3,]
clust[clust$Group==4,]
```

```{r}
#The best cluster is chosen using mean ratings.
mean(clust[clust$Group==1,"rating"])
mean(clust[clust$Group==2,"rating"])
mean(clust[clust$Group==3,"rating"])
mean(clust[clust$Group==4,"rating"])
```

#According to the results above, cluster 1 may be selected as it is the highest.
#Hence, Group 1 can be considered as the healthy diet cluster.
